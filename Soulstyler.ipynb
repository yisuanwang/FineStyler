{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/data15/chenjh2309/Soulstyler/Soulstyler.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224169722d636a682d31342d6430312d387833303930227d/data15/chenjh2309/Soulstyler/Soulstyler.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpredict\u001b[39;00m \u001b[39mimport\u001b[39;00m getMask\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224169722d636a682d31342d6430312d387833303930227d/data15/chenjh2309/Soulstyler/Soulstyler.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224169722d636a682d31342d6430312d387833303930227d/data15/chenjh2309/Soulstyler/Soulstyler.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'predict'"
     ]
    }
   ],
   "source": [
    "from tools.predict import getMask\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import style.utils as utils\n",
    "# from style.CLIPstyler import getStyleImg\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import style.StyleNet as StyleNet\n",
    "import style.utils as utils\n",
    "import torch.nn.functional as F\n",
    "from style.template import imagenet_templates\n",
    "import PIL \n",
    "from torchvision import utils as vutils\n",
    "from torchvision.transforms.functional import adjust_contrast\n",
    "import utils.config as config\n",
    "import random\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "config_path = \"./config/refcoco+/test.yaml\"\n",
    "model_pth = \"./best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaskImg(img,config_path,model_pth,sent=None,isMask=False,):\n",
    "    if not isMask:\n",
    "        img_style1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_style2 = img_style1/255.0\n",
    "        img_style3 = np.transpose(img_style2, (2,0,1))\n",
    "        img_style4 = torch.Tensor(img_style3)\n",
    "        img_style = torch.unsqueeze(img_style4, 0)\n",
    "    \n",
    "        mask0 = getMask(img,sent,config_path,model_pth)\n",
    "        mask1 = np.stack((mask0, mask0,mask0), axis=2)\n",
    "        mask_img = np.array(mask1*255, dtype=np.uint8)\n",
    "        return mask_img\n",
    "    else:\n",
    "        return img \n",
    "\n",
    "def getCVImg2Torch(img):\n",
    "    img_style1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_style2 = img_style1/255.0\n",
    "    img_style3 = np.transpose(img_style2, (2,0,1))\n",
    "    img_style4 = torch.Tensor(img_style3)\n",
    "    img_style = torch.unsqueeze(img_style4, 0)\n",
    "    return img_style\n",
    "\n",
    "def load_image(img,mode=\"PLT\"):\n",
    "    if mode == \"CV\":\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        ])   \n",
    "    image = transform(img)[:3, :, :].unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "def squeeze_convert(tensor):\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze(0)\n",
    "    image = torch.Tensor(image)\n",
    "    return image\n",
    "\n",
    "def img_normalize(image,device):\n",
    "    mean=torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "    std=torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "    mean = mean.view(1,-1,1,1)\n",
    "    std = std.view(1,-1,1,1)\n",
    "\n",
    "    image = (image-mean)/std\n",
    "    return image\n",
    "\n",
    "def clip_normalize(image,device):\n",
    "    image = F.interpolate(image,size=224,mode='bicubic')\n",
    "    mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(device)\n",
    "    std=torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(device)\n",
    "    mean = mean.view(1,-1,1,1)\n",
    "    std = std.view(1,-1,1,1)\n",
    "\n",
    "    image = (image-mean)/std\n",
    "    return image\n",
    "\n",
    "def clip_normalize2(image,device):\n",
    "    image = F.interpolate(image,size=224,mode='bicubic')\n",
    "    return image\n",
    "\n",
    "def get_image_prior_losses(inputs_jit):\n",
    "    diff1 = inputs_jit[:, :, :, :-1] - inputs_jit[:, :, :, 1:]\n",
    "    diff2 = inputs_jit[:, :, :-1, :] - inputs_jit[:, :, 1:, :]\n",
    "    diff3 = inputs_jit[:, :, 1:, :-1] - inputs_jit[:, :, :-1, 1:]\n",
    "    diff4 = inputs_jit[:, :, :-1, :-1] - inputs_jit[:, :, 1:, 1:]\n",
    "\n",
    "    loss_var_l2 = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
    "    \n",
    "    return loss_var_l2\n",
    "\n",
    "def getClipFeature(image,clip_model):\n",
    "    image = F.interpolate(image,size=224,mode='bicubic')\n",
    "    image = clip_model.encode_image(image.to(device))\n",
    "    image = image.mean(axis=0, keepdim=True)\n",
    "    image /= image.norm(dim=-1, keepdim=True)\n",
    "    return image\n",
    "\n",
    "def getVggFeature(image,device,VGG):\n",
    "    return utils.get_features(img_normalize(image,device), VGG)\n",
    "\n",
    "def getLoss(text_feature,img_feature):\n",
    "    return 1-torch.cosine_similarity(text_feature, img_feature)\n",
    "\n",
    "def getCropImgAndFeature(img,mask,target,clip_model,size=128,batch=64,pot_part=0.9,sizePose=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    img, mask, target = img.to(device), mask.to(device), target.to(device)\n",
    "    \n",
    "    back_crop, pot_crop ,pot_aug,extra_pot = [], [],[],[]\n",
    "    cropper = transforms.RandomCrop(size)\n",
    "    augment = transforms.Compose([\n",
    "        transforms.RandomPerspective(fill=0, p=1,distortion_scale=0.5),\n",
    "        transforms.Resize(400)\n",
    "    ])\n",
    "\n",
    "    max_iterations = 20000  \n",
    "    iteration = 0\n",
    "\n",
    "    while len(pot_crop)<batch and iteration < max_iterations :\n",
    "        iteration += 1\n",
    "        if sizePose:\n",
    "            (i, j, h, w) = sizePose\n",
    "        else:\n",
    "            (i, j, h, w) = cropper.get_params(squeeze_convert(mask), (size, size))\n",
    "        mask_crop = transforms.functional.crop(mask, i, j, h, w)\n",
    "        img_crop = transforms.functional.crop(img, i, j, h, w)\n",
    "        target_crop = transforms.functional.crop(target, i, j, h, w)\n",
    "        \n",
    "        if int(mask_crop[0].sum())/(3*size*size) >= 0.8:\n",
    "            if len(pot_crop)<batch :\n",
    "                pot_crop.append(img_crop)\n",
    "                pot_aug.append(augment(target_crop))\n",
    "    \n",
    "    pot_allCrop = torch.cat(pot_crop,dim=0).to(device)\n",
    "    pot_all_crop = pot_allCrop\n",
    "    pot_crop_feature = clip_model.encode_image(clip_normalize2(pot_all_crop, device))\n",
    "\n",
    "    while len(back_crop) < batch and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        (i, j, h, w) = cropper.get_params(squeeze_convert(mask), (size, size))\n",
    "        mask_crop = transforms.functional.crop(mask, i, j, h, w)\n",
    "        img_crop = transforms.functional.crop(img, i, j, h, w)\n",
    "        target_crop = transforms.functional.crop(target, i, j, h, w)\n",
    "        if int(mask_crop[0].sum())/(3*size*size) < 0.1:\n",
    "            img_crop_feature = clip_model.encode_image(clip_normalize2(img_crop,device))\n",
    "            cos = (1- torch.cosine_similarity(img_crop_feature, pot_crop_feature))\n",
    "            if torch.numel(cos[cos>0.12]) > 0.8*batch:\n",
    "                back_crop.append([target_crop,img_crop])\n",
    "        \n",
    "    while len(extra_pot) < 0.1*batch and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        (i, j, h, w) = cropper.get_params(squeeze_convert(mask), (size, size))\n",
    "        mask_crop = transforms.functional.crop(mask, i, j, h, w)\n",
    "        img_crop = transforms.functional.crop(img, i, j, h, w)\n",
    "        target_crop = transforms.functional.crop(target, i, j, h, w)\n",
    "        if int(mask_crop[0].sum())/(3*size*size) < 0.1:\n",
    "            img_crop_feature = clip_model.encode_image(clip_normalize2(img_crop,device))\n",
    "            cos = (1- torch.cosine_similarity(img_crop_feature, pot_crop_feature))\n",
    "            if torch.numel(cos[cos<0.06]) > 0.2*batch:\n",
    "                extra_pot.append(augment(target_crop))\n",
    "                pot_aug.append(augment(target_crop))\n",
    "\n",
    "    return pot_aug, back_crop\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def getTotalLoss(args, content_features, text_features, source_features, text_source, target, device, VGG, clip_model, img, mask):\n",
    "    target_features = utils.get_features(img_normalize(target,device), VGG)\n",
    "    content_loss = 0\n",
    "    content_loss += torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)\n",
    "    content_loss += torch.mean((target_features['conv5_2'] - content_features['conv5_2']) ** 2)\n",
    "\n",
    "    cropper = transforms.Compose([\n",
    "        transforms.RandomCrop(args.crop_size)\n",
    "    ])\n",
    "    augment = transforms.Compose([\n",
    "        transforms.RandomPerspective(fill=0, p=1,distortion_scale=0.5),\n",
    "        transforms.Resize(224)\n",
    "    ])\n",
    "\n",
    "    loss_patch=0 \n",
    "    img_crop, back_crop = getCropImgAndFeature(img, mask, target, clip_model, size=64, batch=64, pot_part=args.pot_part, sizePose=None)\n",
    "    img_crop = torch.cat(img_crop,dim=0)\n",
    "    img_aug = img_crop\n",
    "\n",
    "    image_features = clip_model.encode_image(clip_normalize(img_aug,device))\n",
    "    image_features /= (image_features.clone().norm(dim=-1, keepdim=True))\n",
    "    img_direction = (image_features-source_features)\n",
    "    img_direction /= img_direction.clone().norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_direction = (text_features-text_source).repeat(image_features.size(0),1)\n",
    "    text_direction /= text_direction.norm(dim=-1, keepdim=True)\n",
    "    loss_temp = (1- torch.cosine_similarity(img_direction, text_direction, dim=1))\n",
    "    loss_temp[loss_temp<args.thresh] =0\n",
    "    loss_patch+=loss_temp.mean()\n",
    "    \n",
    "    loss_back = 0\n",
    "    lossToBack = torch.nn.MSELoss()\n",
    "    for i in back_crop:\n",
    "        a = i[0]\n",
    "        b = i[1]\n",
    "        loss_back += lossToBack(a, b)\n",
    "\n",
    "    loss_glob=0\n",
    "    \n",
    "    reg_tv = args.lambda_tv*get_image_prior_losses(target)\n",
    "    total_loss = args.lambda_patch*loss_patch + args.lambda_c * content_loss+ reg_tv+ args.lambda_dir*loss_glob + args.lambda_c * loss_back\n",
    "\n",
    "    detail_loss = {\n",
    "        \"loss_patch\":loss_patch,\n",
    "        \"content_loss\":content_loss,\n",
    "        \"reg_tv\":reg_tv,\n",
    "        \"loss_glob\":loss_glob,\n",
    "        \"loss_back\":loss_back,\n",
    "    }\n",
    "    \n",
    "    return total_loss,detail_loss\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def img_denormalize(image,device):\n",
    "    mean=torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "    std=torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "    mean = mean.view(1,-1,1,1)\n",
    "    std = std.view(1,-1,1,1)\n",
    "\n",
    "    image = image*std +mean\n",
    "    return image\n",
    "\n",
    "def img_normalize(image,device):\n",
    "    mean=torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "    std=torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "    mean = mean.view(1,-1,1,1)\n",
    "    std = std.view(1,-1,1,1)\n",
    "\n",
    "    image = (image-mean)/std\n",
    "    return image\n",
    "\n",
    "def clip_normalize(image,device):\n",
    "    image = F.interpolate(image,size=224,mode='bicubic')\n",
    "    mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(device)\n",
    "    std=torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(device)\n",
    "    mean = mean.view(1,-1,1,1)\n",
    "    std = std.view(1,-1,1,1)\n",
    "\n",
    "    image = (image-mean)/std\n",
    "    return image\n",
    "\n",
    "    \n",
    "def get_image_prior_losses(inputs_jit):\n",
    "    diff1 = inputs_jit[:, :, :, :-1] - inputs_jit[:, :, :, 1:]\n",
    "    diff2 = inputs_jit[:, :, :-1, :] - inputs_jit[:, :, 1:, :]\n",
    "    diff3 = inputs_jit[:, :, 1:, :-1] - inputs_jit[:, :, :-1, 1:]\n",
    "    diff4 = inputs_jit[:, :, :-1, :-1] - inputs_jit[:, :, 1:, 1:]\n",
    "\n",
    "    loss_var_l2 = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
    "    \n",
    "    return loss_var_l2\n",
    "\n",
    "def compose_text_with_templates(text: str, templates=imagenet_templates) -> list:\n",
    "    return [template.format(text) for template in templates]\n",
    "\n",
    "\n",
    "def getStyleImg(config_path,content_image,source=\"a Photo\",prompt=\"a Photo\",seed=42,get_total_loss=getTotalLoss ,mask=None,save_epoch=False,path=''):\n",
    "    setup_seed(seed)\n",
    "    args = config.load_cfg_from_cfg_file(config_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    content_image = content_image.to(device)\n",
    "    VGG = models.vgg19(pretrained=True).features\n",
    "    VGG.to(device)\n",
    "    for parameter in VGG.parameters():\n",
    "        parameter.requires_grad_(False)\n",
    "    \n",
    "    content_features = utils.get_features(img_normalize(content_image,device), VGG)\n",
    "    target = content_image.clone().requires_grad_(True).to(device)\n",
    "    \n",
    "    style_net = StyleNet.UNet()\n",
    "    style_net.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(style_net.parameters(), lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "    steps = args.max_step\n",
    "\n",
    "    output_image = content_image\n",
    "    m_cont = torch.mean(content_image,dim=(2,3),keepdim=False).squeeze(0)\n",
    "    m_cont = [m_cont[0].item(),m_cont[1].item(),m_cont[2].item()]\n",
    "\n",
    "    cropper = transforms.Compose([\n",
    "        transforms.RandomCrop(args.crop_size)\n",
    "    ])\n",
    "    augment = transforms.Compose([\n",
    "        transforms.RandomPerspective(fill=0, p=1,distortion_scale=0.5),\n",
    "        transforms.Resize(224)\n",
    "    ])\n",
    "\n",
    "    clip_model, preprocess = clip.load('ViT-B/32', device, jit=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        template_text = compose_text_with_templates(prompt, imagenet_templates)\n",
    "        tokens = clip.tokenize(template_text).to(device)\n",
    "        text_features = clip_model.encode_text(tokens).detach()\n",
    "        text_features = text_features.mean(axis=0, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        template_source = compose_text_with_templates(source, imagenet_templates)\n",
    "        tokens_source = clip.tokenize(template_source).to(device)\n",
    "        text_source = clip_model.encode_text(tokens_source).detach()\n",
    "        text_source = text_source.mean(axis=0, keepdim=True)\n",
    "        text_source /= text_source.norm(dim=-1, keepdim=True)\n",
    "        source_features = clip_model.encode_image(clip_normalize(content_image,device))\n",
    "        source_features /= (source_features.clone().norm(dim=-1, keepdim=True))\n",
    "    \n",
    "    for epoch in range(0, steps+1):\n",
    "        scheduler.step()\n",
    "        target = style_net(content_image,use_sigmoid=True).to(device)\n",
    "        target.requires_grad_(True)\n",
    "        ###############\n",
    "        total_loss, detail_loss = get_total_loss(args,content_features,text_features,source_features,text_source,target,device,VGG,clip_model,content_image,mask)\n",
    "        ###############\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 ==0:\n",
    "            print('======={}/{}========'.format(epoch, steps+1))\n",
    "            print('Total loss: ', total_loss.item())\n",
    "            for k,v in detail_loss.items():\n",
    "                print(f\"{k}:{v}\")\n",
    "            if save_epoch :\n",
    "                output_image = target.clone()\n",
    "                output_image = torch.clamp(output_image,0,1)\n",
    "                output_image = adjust_contrast(output_image,1.5)\n",
    "                plt.imshow(utils.im_convert2(output_image))\n",
    "                plt.show()\n",
    "            \n",
    "    output_image = target.clone()\n",
    "    output_image = torch.clamp(output_image,0,1)\n",
    "    output_image = adjust_contrast(output_image,1.5)\n",
    "    output_image = utils.im_convert2(output_image)\n",
    "    return output_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StyleProcess(img_path, cris_prompt, style_prompt, seed, save_epoch=False, size=128, pot_part=0.8):\n",
    "    tmp_cris = cris_prompt.replace('.','').replace(' ','_')\n",
    "    tmp_style = style_prompt.replace('.','').replace(' ','_')\n",
    "    \n",
    "    base_path = f'./outputs/size={size}/pot_part={pot_part}/{tmp_cris}/seed={seed}_{tmp_style}'\n",
    "    img_output_image_path = os.path.join(base_path, 'ori_img.png')\n",
    "    mask_output_image_path = os.path.join(base_path, 'mask_img.png')\n",
    "    result_output_image_path = os.path.join(base_path, 'result_img.png')\n",
    "    result_epoch_output_image_path = os.path.join(base_path, 'epoch/')\n",
    "\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "    \n",
    "    if save_epoch and not os.path.exists(result_epoch_output_image_path):\n",
    "        os.makedirs(result_epoch_output_image_path)\n",
    "\n",
    "    if os.path.exists(result_output_image_path):\n",
    "        print(f\"File '{result_output_image_path}' already exists. Exiting function.\")\n",
    "        return\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    mask = getMaskImg(img,config_path,model_pth,cris_prompt,isMask=False)\n",
    "\n",
    "    img = load_image(img,mode=\"CV\")\n",
    "    mask = load_image(mask)\n",
    "\n",
    "    img = img.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # show ori image\n",
    "    plt.imshow(utils.im_convert2(img))\n",
    "    plt.show()\n",
    "    # show mask image\n",
    "    plt.imshow(utils.im_convert2(mask))\n",
    "    plt.show()\n",
    "\n",
    "    # save_image(img, img_output_image_path)\n",
    "    # save_image(mask, mask_output_image_path)\n",
    "\n",
    "    output_image = getStyleImg(\n",
    "        config_path, img, source=\"a Photo\",\n",
    "        prompt=style_prompt,\n",
    "        seed=seed,\n",
    "        get_total_loss=getTotalLoss,\n",
    "        mask=mask,\n",
    "        save_epoch=save_epoch,\n",
    "        path = result_epoch_output_image_path\n",
    "    ).to(device)\n",
    "\n",
    "    # result\n",
    "    plt.figure(figsize=(20, 20))#6，8分别对应宽和高\n",
    "    plt.imshow(utils.im_convert2(output_image))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receiving colab's restricted GPU and LLaMA-2 licensing policy, we only provide Splitting Stylized Instruction, i.e. Stylized Content and Stylized Objects, after LLM segmentation in this colab. The full code containing the LLM split prompt will be described in github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_prompt_list = [\n",
    "    {'style':\"white wool\", 'seed':2},\n",
    "    {'style':\"a sketch with crayon\", 'seed':3},\n",
    "    {'style':\"oil painting of flowers\", 'seed':7},\n",
    "    {'style':'pop art of night city', 'seed':6},\n",
    "    {'style':\"Starry Night by Vincent van gogh\", 'seed':0},\n",
    "    {'style':\"neon light\", 'seed':5},\n",
    "    {'style':\"mosaic\", 'seed':4},\n",
    "    {'style':\"green crystal\", 'seed':1},\n",
    "    {'style':\"Underwater\", 'seed':0},\n",
    "    {'style':\"fire\", 'seed':0},\n",
    "    {'style':'a graffiti style painting', 'seed':2},\n",
    "    {'style':'The great wave off kanagawa by Hokusai', 'seed':0},\n",
    "    {'style':'Wheatfield by Vincent van gogh', 'seed':2},\n",
    "    {'style':'a Photo of white cloud', 'seed':3},\n",
    "    {'style':'a monet style underwater', 'seed':3},\n",
    "    {'style':'A fauvism style painting', 'seed':2}\n",
    "]\n",
    "\n",
    "input_data  = [\n",
    "    {\n",
    "        'img_path' : \"./testimg/ship.jpg\",\n",
    "        'cris_prompt' : \"A white sailboat with three blue sails floating on the sea\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/911.jpg\",\n",
    "        'cris_prompt' : \"a plane\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/1.jpg\",\n",
    "        'cris_prompt' : \"a flower\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/house.jpg\",\n",
    "        'cris_prompt' : \"a house\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/people.jpg\",\n",
    "        'cris_prompt' : \"the face\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/Napoleon.jpg\",\n",
    "        'cris_prompt' : \"a White War Horse\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/apple.png\",\n",
    "        'cris_prompt' : \"a red apple\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/bigship.png\",\n",
    "        'cris_prompt' : \"White Large Luxury Cruise Ship\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/car.png\",\n",
    "        'cris_prompt' : \"White sports car.\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/lena.png\",\n",
    "        'cris_prompt' : \"A woman's face.\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/mountain.png\",\n",
    "        'cris_prompt' : \"mountain peak\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/tjl.jpeg\",\n",
    "        'cris_prompt' : \"The White House at the Taj Mahal\"\n",
    "    },\n",
    "    {\n",
    "        'img_path' : \"./testimg/man.jpg\",\n",
    "        'cris_prompt' : \"The Men's face\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "StyleProcess(img_path=input_data[0]['img_path'], \n",
    "                            cris_prompt=input_data[0]['cris_prompt'], \n",
    "                            style_prompt=style_prompt_list[2]['style'], \n",
    "                            seed=style_prompt_list[2]['seed'], \n",
    "                            save_epoch=True, \n",
    "                            size=128, \n",
    "                            pot_part=0.8)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soulstyler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
